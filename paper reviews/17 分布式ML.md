

| Reviewer **(R2)**                                            |
| ------------------------------------------------------------ |
| Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis |
| 2019                                                         |
| CS  341                                                      |

## summary

1. Survey results from 227 papers in the area of distributed deep learning. It comprehensively surveys the challenges and techniques for parallelizing Deep Neural Networks.
2. Their comprehensive analysis covers different devices, including FPGA, GPU, and specialized devices, from single to multiple nodes.
3. They summarize the parallelisms and their features, including model parallelism, pipeline parallelism, data parallelism, and hybrid parallelism.

## advantage

1. They discuss communication optimization techniques, including different options for optimizing updates, quantization, and sparsification.
2. They introduce the evolution of the consistency of parameters (and models).   The consistency of parameters can be achieved through centralized or decentralized methods. 
3. They present trends in DNN architectures and their implications on parallelization strategies.

## drawback

1. The paper does not provide a detailed comparison of different parallelization techniques.
1. It may cover only some possible approaches or techniques for distributed deep learning.
1. It provides less guidance on optimizing performance or efficiency in specific scenarios.

## Justification

1. Deep learning primarily relies on distributed memory, and the number of nodes used increases from a lower number. The communication mode for Deep Learning research converges towards MPI, and old methods like Mapreduce and Spark are not used.
2. Distributed data parallelism involves updating parameters, and they discuss this process.
3. They delve into the topic of Hyperparameter and Architecture search.
4. Their focus is on the application of Neural Code Comprehension.
