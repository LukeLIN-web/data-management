

| Reviewer **(R2)**                                            |
| ------------------------------------------------------------ |
| Compressed Communication for Distributed Deep Learning: Survey and Quantitative Evaluation |
| ICDCS 2021  41st IEEE Int. Conf. Distributed Computing Systems |
| CS  341                                                      |

## summary

1. This paper summary the compressed communication methods for DNN training.
1. provides a thorough quantitative evaluation of 16 compressed communication methods for DNN training, implemented using a unified framework and API on TensorFlow and PyTorch.
1. proposes a unified framework and API that allows for consistent and easy implementation of compressed communication

## advantage

1.  GRACE supports both TensorFlow and Pytorch, and offers easy integration with training scripts. This simplifies the process of implemeting and comparing across compression methods.
1. The evaluation results provide the relative performance among different compression techniques under various conditions.
1. conclude that this area of research would benefit from increased attention.

## drawback

1. some of the existing work on compressed communication for DNN training suffers from shortcomings such as unrealistic assumptions or non-standard benchmarks.
1. it may not cover all existing compressed communication methods
1. they may not be generalizable to all possible DNN architectures and datasets

## Justification

1. Quantization involves reducing the precision of weights and activations to reduce the number of bits required to represent them. Sparsification involves setting some weights or activations to zero to reduce the amount of information that needs to be communicated. Hybrid methods combine both quantization and sparsification techniques. Low-rank methods aim to approximate weight matrices with low-rank matrices, which can also reduce the amount of information that needs to be communicated.
1. use both convolutional and recurrent DNNs, as well as a variety of datasets and system configu- rations, to perform thorough experimental evaluation and report metrics that include accuracy, throughput, scalability, communication volume and wall-time
3. The promising area:
    • Employing compression in network-constrained scenarios such as geo-distributed or federated learning settings.
    • Offloading the compression overhead to a hardware accelerator (e.g., FPGA NICs [47]).
    • Revisiting the underlying communication libraries to work efficiently with compressed data (e.g., NCCL Allreduce for sparse vectors).

为什么要写这个框架 ?所有人都用自己的实现, 不和别人比较, 

sketchML 更慢收敛.

QSGD , 有小概率到高的位, 因为这样估计bias 小或者没有bais,   不是四舍五入. 

low rank decomposition, 把一个矩阵分解成两个向量. 

这个框架还有memory function.

压缩的方法越多, 模型质量越好. Randk 0.01 , 发送的很少, acc也高, . 还有 thresh 0.01效果很好,吞吐量较高. 

很多方法, 吞吐量都不如 baseline.

小模型上, 压缩的计算量太大, 反而不如普通方法. 

现在很多网络400GB/s ,  这些压缩方法都没用了. 不过LLM可能让压缩更重要.  

现在实际的压缩算法需要很小心调整, 比如compensate error.

工作量巨大, 花了两年写这个论文,  很多作者都不回复邮件. 



